{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time   # time1 = time.time(); print('Time taken: {:.1f} seconds'.format(time.time() - time1))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42   # set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up image augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    "    #zoom_range=0.3\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding.\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.Dense(800, activation=\"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(keras.layers.Dense(400, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "# initialize our initial learning rate and # of epochs to train for\n",
    "INIT_LR = 0.01\n",
    "EPOCHS = 20\n",
    " \n",
    "# compile the model using SGD as our optimizer and categorical cross-entropy loss\n",
    "# (you'll want to use binary_crossentropy for 2-class classification)\n",
    "print(\"[INFO] training network...\")\n",
    "opt = SGD(lr=INIT_LR)   # Stochastic Gradient Descent (SGD) optimizer\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.8531 - accuracy: 0.3342 - val_loss: 1.8263 - val_accuracy: 0.3463\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6594 - accuracy: 0.4094 - val_loss: 1.5925 - val_accuracy: 0.4309\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5746 - accuracy: 0.4420 - val_loss: 1.5704 - val_accuracy: 0.4434\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5139 - accuracy: 0.4623 - val_loss: 1.5022 - val_accuracy: 0.4604\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4658 - accuracy: 0.4780 - val_loss: 1.4870 - val_accuracy: 0.4670\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4242 - accuracy: 0.4939 - val_loss: 1.4836 - val_accuracy: 0.4736\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.3874 - accuracy: 0.5085 - val_loss: 1.4736 - val_accuracy: 0.4717\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3522 - accuracy: 0.5215 - val_loss: 1.5093 - val_accuracy: 0.4564\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3229 - accuracy: 0.5313 - val_loss: 1.4840 - val_accuracy: 0.4825\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2942 - accuracy: 0.5404 - val_loss: 1.3952 - val_accuracy: 0.5067\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2645 - accuracy: 0.5533 - val_loss: 1.4282 - val_accuracy: 0.4942\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2374 - accuracy: 0.5609 - val_loss: 1.5463 - val_accuracy: 0.4599\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2148 - accuracy: 0.5703 - val_loss: 1.3615 - val_accuracy: 0.5164\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1878 - accuracy: 0.5789 - val_loss: 1.3729 - val_accuracy: 0.5145\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1637 - accuracy: 0.5879 - val_loss: 1.3959 - val_accuracy: 0.5081\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1403 - accuracy: 0.5957 - val_loss: 1.3520 - val_accuracy: 0.5264\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1136 - accuracy: 0.6052 - val_loss: 1.3666 - val_accuracy: 0.5231\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0912 - accuracy: 0.6143 - val_loss: 1.3855 - val_accuracy: 0.5185\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.0682 - accuracy: 0.6217 - val_loss: 1.4361 - val_accuracy: 0.5023\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0431 - accuracy: 0.6327 - val_loss: 1.3852 - val_accuracy: 0.5186\n",
      "Time taken: 320.8 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the neural network on training data set\n",
    "# batch_size (32) controls the size of each group of data to pass through the network. \n",
    "# Larger GPUs would be able to accommodate larger batch sizes (eg. 64)\n",
    "time1 = time.time()   # to measure time taken\n",
    "H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS, batch_size=32)\n",
    "print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
